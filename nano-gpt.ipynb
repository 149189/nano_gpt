{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9a3e549",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-18T02:01:07.370623Z",
     "iopub.status.busy": "2025-02-18T02:01:07.370276Z",
     "iopub.status.idle": "2025-02-18T02:01:08.112775Z",
     "shell.execute_reply": "2025-02-18T02:01:08.111784Z"
    },
    "papermill": {
     "duration": 0.749352,
     "end_time": "2025-02-18T02:01:08.114303",
     "exception": false,
     "start_time": "2025-02-18T02:01:07.364951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/tiny-shakespeare-dataset/tiny shakespeare.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d53a92d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T02:01:08.122976Z",
     "iopub.status.busy": "2025-02-18T02:01:08.122580Z",
     "iopub.status.idle": "2025-02-18T02:01:12.436645Z",
     "shell.execute_reply": "2025-02-18T02:01:12.435946Z"
    },
    "papermill": {
     "duration": 4.319931,
     "end_time": "2025-02-18T02:01:12.438315",
     "exception": false,
     "start_time": "2025-02-18T02:01:08.118384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import os\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "964b214e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T02:01:12.447118Z",
     "iopub.status.busy": "2025-02-18T02:01:12.446738Z",
     "iopub.status.idle": "2025-02-18T02:01:12.466566Z",
     "shell.execute_reply": "2025-02-18T02:01:12.465741Z"
    },
    "papermill": {
     "duration": 0.02552,
     "end_time": "2025-02-18T02:01:12.467898",
     "exception": false,
     "start_time": "2025-02-18T02:01:12.442378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load tiny_shakespeare.txt\n",
    "with open(\"/kaggle/input/tiny-shakespeare-dataset/tiny shakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6798a4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T02:01:12.476007Z",
     "iopub.status.busy": "2025-02-18T02:01:12.475754Z",
     "iopub.status.idle": "2025-02-18T02:01:12.480027Z",
     "shell.execute_reply": "2025-02-18T02:01:12.479181Z"
    },
    "papermill": {
     "duration": 0.009853,
     "end_time": "2025-02-18T02:01:12.481426",
     "exception": false,
     "start_time": "2025-02-18T02:01:12.471573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenizer (BPE)\n",
    "def train_bpe_tokenizer(texts, vocab_size=5000):\n",
    "    os.makedirs(\"bpe_tokenizer\", exist_ok=True)\n",
    "\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "    tokenizer.pre_tokenizer = ByteLevel()\n",
    "    trainer = BpeTrainer(vocab_size=vocab_size, special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"])\n",
    "    tokenizer.train_from_iterator(texts, trainer)\n",
    "    \n",
    "    # Save as tokenizer.json (needed for PreTrainedTokenizerFast)\n",
    "    tokenizer.save(\"bpe_tokenizer/tokenizer.json\")\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d333d4e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T02:01:12.489549Z",
     "iopub.status.busy": "2025-02-18T02:01:12.489311Z",
     "iopub.status.idle": "2025-02-18T02:01:13.437507Z",
     "shell.execute_reply": "2025-02-18T02:01:13.436775Z"
    },
    "papermill": {
     "duration": 0.95403,
     "end_time": "2025-02-18T02:01:13.439206",
     "exception": false,
     "start_time": "2025-02-18T02:01:12.485176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training new BPE tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# Load or train tokenizer\n",
    "if not os.path.exists(\"bpe_tokenizer/tokenizer.json\"):\n",
    "    print(\"Training new BPE tokenizer...\")\n",
    "    train_bpe_tokenizer([shakespeare_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45ed39ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T02:01:13.447930Z",
     "iopub.status.busy": "2025-02-18T02:01:13.447652Z",
     "iopub.status.idle": "2025-02-18T02:01:13.462125Z",
     "shell.execute_reply": "2025-02-18T02:01:13.461439Z"
    },
    "papermill": {
     "duration": 0.02024,
     "end_time": "2025-02-18T02:01:13.463433",
     "exception": false,
     "start_time": "2025-02-18T02:01:13.443193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "bpe_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"/kaggle/working/bpe_tokenizer/tokenizer.json\",\n",
    "    unk_token=\"<unk>\", pad_token=\"<pad>\", cls_token=\"<s>\", sep_token=\"</s>\", mask_token=\"<mask>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85343004",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T02:01:13.471576Z",
     "iopub.status.busy": "2025-02-18T02:01:13.471297Z",
     "iopub.status.idle": "2025-02-18T02:01:13.476592Z",
     "shell.execute_reply": "2025-02-18T02:01:13.475756Z"
    },
    "papermill": {
     "duration": 0.010589,
     "end_time": "2025-02-18T02:01:13.477795",
     "exception": false,
     "start_time": "2025-02-18T02:01:13.467206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, text, context_length=256):\n",
    "        global bpe_tokenizer  # Ensure global tokenizer is loaded\n",
    "\n",
    "        tokens = bpe_tokenizer(text, return_tensors=\"pt\")[\"input_ids\"].squeeze()\n",
    "        \n",
    "        # Ensure data is long enough\n",
    "        if tokens.numel() < context_length:\n",
    "            raise ValueError(f\"Tokenized text is too short! Only {tokens.numel()} tokens available.\")\n",
    "\n",
    "        self.data = tokens\n",
    "        self.context_length = context_length\n",
    "        print(f\"Tokenized data size: {self.data.shape}\")  # Check the size\n",
    "        print(f\"First few tokens: {self.data[:10]}\")  # Inspect the first tokens\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(0, len(self.data) - self.context_length)  # Prevent negative length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.data[idx:idx+self.context_length],\n",
    "                self.data[idx+1:idx+self.context_length+1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f4a767e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T02:01:13.485905Z",
     "iopub.status.busy": "2025-02-18T02:01:13.485616Z",
     "iopub.status.idle": "2025-02-18T02:01:13.498207Z",
     "shell.execute_reply": "2025-02-18T02:01:13.497602Z"
    },
    "papermill": {
     "duration": 0.018105,
     "end_time": "2025-02-18T02:01:13.499463",
     "exception": false,
     "start_time": "2025-02-18T02:01:13.481358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GPT Model Configuration\n",
    "class GPTConfig:\n",
    "    def __init__(self, vocab_size, context_length=256, \n",
    "                 n_layers=12, n_heads=12, d_model=768, d_ff=3072, dropout=0.1):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.context_length = context_length\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout = dropout\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.position_embedding = nn.Parameter(torch.zeros(1, config.context_length, config.d_model))\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(config) for _ in range(config.n_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(config.d_model)\n",
    "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        tok_emb = self.token_embedding(x)\n",
    "        pos_emb = self.position_embedding[:, :T, :]\n",
    "        x = tok_emb + pos_emb\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.d_model)\n",
    "        self.attn = MultiHeadAttention(config)\n",
    "        self.ln2 = nn.LayerNorm(config.d_model)\n",
    "        self.ffn = FeedForward(config)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.d_model % config.n_heads == 0\n",
    "        self.n_heads = config.n_heads\n",
    "        self.d_head = config.d_model // config.n_heads\n",
    "        self.W_qkv = nn.Linear(config.d_model, 3 * config.d_model, bias=False)\n",
    "        self.W_o = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.W_qkv(x).reshape(B, T, 3, self.n_heads, self.d_head).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) / (self.d_head ** 0.5)\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        attn_probs = self.dropout(attn_probs)\n",
    "        attn_output = (attn_probs @ v).transpose(1, 2).reshape(B, T, C)\n",
    "        return self.W_o(attn_output)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config.d_model, config.d_ff)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc2 = nn.Linear(config.d_ff, config.d_model)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.fc2(self.gelu(self.fc1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d1f9336",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T02:01:13.507359Z",
     "iopub.status.busy": "2025-02-18T02:01:13.507138Z",
     "iopub.status.idle": "2025-02-18T02:01:19.853843Z",
     "shell.execute_reply": "2025-02-18T02:01:19.852734Z"
    },
    "papermill": {
     "duration": 6.35235,
     "end_time": "2025-02-18T02:01:19.855383",
     "exception": false,
     "start_time": "2025-02-18T02:01:13.503033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized data size: torch.Size([333336])\n",
      "First few tokens: tensor([  69,  485, 1010,   13,   68, 2156,  145, 2561,  616, 2116])\n"
     ]
    }
   ],
   "source": [
    "# Training Setup\n",
    "config = GPTConfig(vocab_size=5000, context_length=256, n_layers=12, n_heads=12, d_model=768, d_ff=3072)\n",
    "model = GPT(config)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), weight_decay=0.1)\n",
    "\n",
    "dataset = ShakespeareDataset(shakespeare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f2a5a93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T02:01:19.864323Z",
     "iopub.status.busy": "2025-02-18T02:01:19.863866Z",
     "iopub.status.idle": "2025-02-18T02:01:19.869182Z",
     "shell.execute_reply": "2025-02-18T02:01:19.868469Z"
    },
    "papermill": {
     "duration": 0.011004,
     "end_time": "2025-02-18T02:01:19.870458",
     "exception": false,
     "start_time": "2025-02-18T02:01:19.859454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def train_model(model, dataset, epochs=10, batch_size=32):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = loss_fn(logits.view(-1, config.vocab_size), y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68aa1f2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T02:01:19.878709Z",
     "iopub.status.busy": "2025-02-18T02:01:19.878441Z",
     "iopub.status.idle": "2025-02-18T02:01:19.883104Z",
     "shell.execute_reply": "2025-02-18T02:01:19.882108Z"
    },
    "papermill": {
     "duration": 0.010267,
     "end_time": "2025-02-18T02:01:19.884381",
     "exception": false,
     "start_time": "2025-02-18T02:01:19.874114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters: 92.90M\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95c840d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T02:01:19.892506Z",
     "iopub.status.busy": "2025-02-18T02:01:19.892283Z",
     "iopub.status.idle": "2025-02-18T02:01:20.976604Z",
     "shell.execute_reply": "2025-02-18T02:01:20.975605Z"
    },
    "papermill": {
     "duration": 1.089901,
     "end_time": "2025-02-18T02:01:20.978095",
     "exception": false,
     "start_time": "2025-02-18T02:01:19.888194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized data size: torch.Size([333336])\n",
      "First few tokens: tensor([  69,  485, 1010,   13,   68, 2156,  145, 2561,  616, 2116])\n",
      "Dataset size: 333080\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "context_length = 256\n",
    "dataset = ShakespeareDataset(shakespeare_text, context_length=context_length)\n",
    "print(f\"Dataset size: {len(dataset)}\")  # Should be > 0\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcbc81c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T02:01:20.986989Z",
     "iopub.status.busy": "2025-02-18T02:01:20.986732Z",
     "iopub.status.idle": "2025-02-18T02:01:21.045698Z",
     "shell.execute_reply": "2025-02-18T02:01:21.044875Z"
    },
    "papermill": {
     "duration": 0.065074,
     "end_time": "2025-02-18T02:01:21.047121",
     "exception": false,
     "start_time": "2025-02-18T02:01:20.982047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "944e4a46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T02:01:21.055602Z",
     "iopub.status.busy": "2025-02-18T02:01:21.055361Z",
     "iopub.status.idle": "2025-02-18T02:01:22.607087Z",
     "shell.execute_reply": "2025-02-18T02:01:22.606288Z"
    },
    "papermill": {
     "duration": 1.557665,
     "end_time": "2025-02-18T02:01:22.608746",
     "exception": false,
     "start_time": "2025-02-18T02:01:21.051081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "config = GPTConfig(vocab_size=5000, context_length=context_length, n_layers=12, n_heads=12, d_model=768, d_ff=3072)\n",
    "model = GPT(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5574589f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T02:01:22.617887Z",
     "iopub.status.busy": "2025-02-18T02:01:22.617572Z",
     "iopub.status.idle": "2025-02-18T02:01:22.622349Z",
     "shell.execute_reply": "2025-02-18T02:01:22.621593Z"
    },
    "papermill": {
     "duration": 0.010638,
     "end_time": "2025-02-18T02:01:22.623619",
     "exception": false,
     "start_time": "2025-02-18T02:01:22.612981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c54d0f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T02:01:22.632216Z",
     "iopub.status.busy": "2025-02-18T02:01:22.631983Z",
     "iopub.status.idle": "2025-02-18T02:01:22.636408Z",
     "shell.execute_reply": "2025-02-18T02:01:22.635801Z"
    },
    "papermill": {
     "duration": 0.009993,
     "end_time": "2025-02-18T02:01:22.637523",
     "exception": false,
     "start_time": "2025-02-18T02:01:22.627530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(model, dataloader, epochs=1):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = loss_fn(logits.view(-1, config.vocab_size), y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8606476",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T02:01:22.645673Z",
     "iopub.status.busy": "2025-02-18T02:01:22.645432Z",
     "iopub.status.idle": "2025-02-18T04:13:09.468706Z",
     "shell.execute_reply": "2025-02-18T04:13:09.467672Z"
    },
    "papermill": {
     "duration": 7906.833212,
     "end_time": "2025-02-18T04:13:09.474480",
     "exception": false,
     "start_time": "2025-02-18T02:01:22.641268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 0.6818\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(model, dataloader, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cab03a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T04:13:09.483285Z",
     "iopub.status.busy": "2025-02-18T04:13:09.483020Z",
     "iopub.status.idle": "2025-02-18T04:13:09.965837Z",
     "shell.execute_reply": "2025-02-18T04:13:09.964796Z"
    },
    "papermill": {
     "duration": 0.488862,
     "end_time": "2025-02-18T04:13:09.967214",
     "exception": false,
     "start_time": "2025-02-18T04:13:09.478352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as nano_gpt_92.90M.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"nano_gpt_92.90M.pth\")\n",
    "print(\"Model saved as nano_gpt_92.90M.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddb9c70",
   "metadata": {
    "papermill": {
     "duration": 0.003557,
     "end_time": "2025-02-18T04:13:09.974906",
     "exception": false,
     "start_time": "2025-02-18T04:13:09.971349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5464385,
     "sourceId": 9061177,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7927.699044,
   "end_time": "2025-02-18T04:13:12.443441",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-18T02:01:04.744397",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
